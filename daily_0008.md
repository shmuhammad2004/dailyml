## On Decision Trees

When I started with machine learning, the first model I built was a Decision Tree.

As a long-time software developer, Decision Trees made perfect sense. It took me little time to understand the basics and build something useful to solve an example problem.

Since then, I always recommend newcomers start with Decision Trees. There are simpler models, but I think they strike a perfect balance between power and complexity.

Which of the following would you categorize as an advantage of using a single Decision Tree?


1. The predictions generated by a Decision Tree are easy to interpret and explain.

2. A Decision Tree is very resistant to overfitting.

3. You can use a Decision Tree to solve regression and classification tasks.

4. A Decision Tree doesn't require tuning to get good predictions.
Very good, almost got it!
As a developer, Decision Trees have always made sense.


## Answer

A Decision Tree is just a bunch of nested conditions that get us to the final solution.

And this property is precisely what makes them very popular: we can look at a prediction and fully understand why the model arrived at that conclusion. In other words, the predictions generated by Decision Trees are easy to interpret, an essential advantage of using Decision Trees over more obscure techniques, like neural networks or Support Vector Machines.

Decision Trees, unfortunately, are prone to overfitting if we don't take careful care of their depth. In other words, unless we ensure our tree doesn't go too deep, it will tend to fit noisy samples and output the wrong prediction. We can control overfitting in a Decision Tree by a process called "pruning."

Remember that while a single Decision Tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from ["To Boost or not to Boost: On the Limits of Boosted Neural Networks"](https://arxiv.org/pdf/2107.13600.pdf):

>[these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting.

Many people relate Decision Trees with classification tasks, but they are also valuable for solving regression tasks. A classification task is when the predicted outcome is a discrete class, while the result of a regression task is a Real number. This flexibility makes Decision Trees very useful.

Finally, we discussed above how Decision Trees are prone to overfit if we aren't careful with their depth. This is just one of the hyperparameters that we can tune. Like with most techniques, Decision Trees require experimentation and tuning to improve the quality of their results.


## Recommended reading


1. Check "To Boost or not to Boost: On the Limits of Boosted Neural Networks", the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.

2. ["Decision tree pruning"](https://en.wikipedia.org/wiki/Decision_tree_pruning) covers the process of pruning a tree to reduce overfitting.

3. Check ["Random Forest"](https://en.wikipedia.org/wiki/Random_forest) for more information about a widely used class of Decision Tree ensembles.

4. [How to Develop a Random Forest Ensemble in Python](https://machinelearningmastery.com/random-forest-ensemble-in-python/)

5. Kaggle: [Decision Trees & Random Forest for Beginners](https://www.kaggle.com/code/faressayah/decision-trees-random-forest-for-beginners)

6. Kaggle [Random Forest Classifier + Feature Importance](https://www.kaggle.com/code/prashant111/random-forest-classifier-feature-importance/notebook) 


